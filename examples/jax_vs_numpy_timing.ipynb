{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing NumPy and JAX Backend Performance for H0LogLikelihood\n",
    "\n",
    "This notebook demonstrates how to select the numerical backend (NumPy or JAX) for the `H0LogLikelihood` function in the `gwsiren` pipeline and provides a basic comparison of their performance. For more comprehensive benchmarking, especially on GPU hardware, please use the `scripts/bench_jax.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure basic logging to see messages from the backend selection\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Import necessary components from gwsiren\n",
    "# Ensure gw-siren-pipeline is installed and accessible in your PYTHONPATH\n",
    "# Typically, if you are in the notebook inside 'examples/', and the package is in '../gw-siren-pipeline'\n",
    "# you might need to adjust sys.path or install the package in editable mode.\n",
    "try:\n",
    "    from gw_siren_pipeline.gwsiren.h0_mcmc_analyzer import (\n",
    "        get_log_likelihood_h0,\n",
    "        DEFAULT_SIGMA_V_PEC,\n",
    "        DEFAULT_C_LIGHT,\n",
    "        DEFAULT_OMEGA_M,\n",
    "        DEFAULT_H0_PRIOR_MIN,\n",
    "        DEFAULT_H0_PRIOR_MAX,\n",
    "        DEFAULT_ALPHA_PRIOR_MIN,\n",
    "        DEFAULT_ALPHA_PRIOR_MAX\n",
    "    )\n",
    "    # CONFIG is loaded when gwsiren.config is imported, which happens internally\n",
    "    # from gw_siren_pipeline.gwsiren import CONFIG # To access CONFIG.backend if needed\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing gwsiren components: {e}\\n\" \\\n",
    "          \"Please ensure 'gw-siren-pipeline' is installed (e.g., 'pip install -e .') \" \\\n",
    "          \"from the 'gw-siren-pipeline' directory and your PYTHONPATH is set correctly.\")\n",
    "    # You might need to add the project root to sys.path if running directly from examples\n",
    "    # import os\n",
    "    # module_path = os.path.abspath(os.path.join('..'))\n",
    "    # if module_path not in sys.path:\n",
    "    #     sys.path.append(module_path)\n",
    "    # from gw_siren_pipeline.gwsiren.h0_mcmc_analyzer import get_log_likelihood_h0, ...\n",
    "\n",
    "# JAX setup (optional, but good practice for consistency)\n",
    "try:\n",
    "    import jax\n",
    "    jax.config.update(\"jax_enable_x64\", True)\n",
    "    JAX_AVAILABLE = True\n",
    "    print(f\"JAX available. Version: {jax.__version__}. Default device: {jax.default_backend()}, Devices: {jax.devices()}\")\n",
    "except ImportError:\n",
    "    JAX_AVAILABLE = False\n",
    "    print(\"JAX not available. JAX-related cells will be skipped or will show NumPy behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation\n",
    "\n",
    "We'll generate a small mock dataset. For more extensive benchmarks, use `scripts/bench_jax.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notebook_mock_data(num_gw_samples: int, num_hosts: int):\n",
    "    \"\"\"Generates simplified mock data for notebook demonstration.\"\"\"\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    dL_gw_samples = rng.normal(loc=700, scale=70, size=num_gw_samples).astype(np.float64)\n",
    "    z_values = rng.uniform(low=0.01, high=0.1, size=num_hosts).astype(np.float64)\n",
    "    mass_proxy_values = rng.lognormal(mean=10, sigma=1, size=num_hosts).astype(np.float64)\n",
    "    mass_proxy_values = np.maximum(mass_proxy_values, 1e-5)\n",
    "    z_err_values = rng.uniform(low=0.001, high=0.002, size=num_hosts).astype(np.float64)\n",
    "    return dL_gw_samples, z_values, mass_proxy_values, z_err_values\n",
    "\n",
    "# Small dataset for quick notebook execution\n",
    "NUM_GW_SAMPLES_NB = 100\n",
    "NUM_HOSTS_NB = 1000 \n",
    "\n",
    "dL_gw, z_hosts, mass_proxy, z_err_hosts = generate_notebook_mock_data(NUM_GW_SAMPLES_NB, NUM_HOSTS_NB)\n",
    "\n",
    "print(f\"Generated mock data: {dL_gw.shape[0]} GW samples, {z_hosts.shape[0]} host galaxies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backend Selection and Likelihood Instantiation\n",
    "\n",
    "The `get_log_likelihood_h0` factory function uses the `backend_preference` argument (`\"auto\"`, `\"numpy\"`, or `\"jax\"`) to determine which numerical backend to use. The `\"auto\"` mode prioritizes JAX if a GPU is available, otherwise it falls back to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for the likelihood function\n",
    "common_params = {\n",
    "    \"dL_gw_samples\": dL_gw,\n",
    "    \"host_galaxies_z\": z_hosts,\n",
    "    \"host_galaxies_mass_proxy\": mass_proxy,\n",
    "    \"host_galaxies_z_err\": z_err_hosts,\n",
    "    \"sigma_v\": DEFAULT_SIGMA_V_PEC,\n",
    "    \"c_val\": DEFAULT_C_LIGHT,\n",
    "    \"omega_m_val\": DEFAULT_OMEGA_M,\n",
    "    \"h0_min\": DEFAULT_H0_PRIOR_MIN,\n",
    "    \"h0_max\": DEFAULT_H0_PRIOR_MAX,\n",
    "    \"alpha_min\": DEFAULT_ALPHA_PRIOR_MIN,\n",
    "    \"alpha_max\": DEFAULT_ALPHA_PRIOR_MAX\n",
    "}\n",
    "\n",
    "# Instantiate with NumPy backend\n",
    "print(\"\\n--- Instantiating NumPy backend ---\")\n",
    "log_L_numpy_instance = get_log_likelihood_h0(**common_params, backend_preference=\"numpy\")\n",
    "print(f\"NumPy likelihood instance: {log_L_numpy_instance}\")\n",
    "print(f\"NumPy backend selected: {log_L_numpy_instance.backend_name}, xp module: {log_L_numpy_instance.xp}\")\n",
    "\n",
    "# Instantiate with JAX backend (if available)\n",
    "log_L_jax_instance = None\n",
    "if JAX_AVAILABLE:\n",
    "    print(\"\\n--- Instantiating JAX backend ---\")\n",
    "    log_L_jax_instance = get_log_likelihood_h0(**common_params, backend_preference=\"jax\")\n",
    "    print(f\"JAX likelihood instance: {log_L_jax_instance}\")\n",
    "    print(f\"JAX backend selected: {log_L_jax_instance.backend_name}, xp module: {log_L_jax_instance.xp}\")\n",
    "    if log_L_jax_instance._jitted_likelihood_core:\n",
    "        print(\"JAX core likelihood function has been JIT-compiled.\")\n",
    "    else:\n",
    "        print(\"JAX core likelihood function was NOT JIT-compiled (e.g. JAX available but no GPU for 'auto', or JIT failed).\")\n",
    "else:\n",
    "    print(\"\\nSkipping JAX backend instantiation as JAX is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Timing Comparison\n",
    "\n",
    "Let's perform a simple timing comparison. Note that for JAX, the first call includes JIT compilation overhead, so warmup calls are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_sample = np.array([70.0, 0.0]) # [H0, alpha_g]\n",
    "num_warmup_calls = 5\n",
    "num_timed_calls = 20\n",
    "\n",
    "def time_evaluations(likelihood_instance, theta, num_warmup, num_timed, backend_name):\n",
    "    if likelihood_instance is None:\n",
    "        print(f\"Skipping timing for {backend_name} as instance is not available.\")\n",
    "        return None\n",
    "    \n",
    "    xp_module = likelihood_instance.xp\n",
    "    theta_device = xp_module.asarray(theta) \n",
    "    \n",
    "    print(f\"\\nWarmup for {backend_name} ({num_warmup} calls)...\")\n",
    "    for _ in range(num_warmup):\n",
    "        res = likelihood_instance(theta_device)\n",
    "        if hasattr(res, 'block_until_ready'): # For JAX\n",
    "            res.block_until_ready()\n",
    "            \n",
    "    print(f\"Timing {backend_name} ({num_timed} calls)...\")\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(num_timed):\n",
    "        res = likelihood_instance(theta_device)\n",
    "        if hasattr(res, 'block_until_ready'): # For JAX\n",
    "            res.block_until_ready()\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    avg_time = total_time / num_timed\n",
    "    print(f\"{backend_name} average time per call: {avg_time:.6f} seconds\")\n",
    "    return avg_time\n",
    "\n",
    "# Time NumPy\n",
    "time_numpy = time_evaluations(log_L_numpy_instance, theta_sample, num_warmup_calls, num_timed_calls, \"NumPy\")\n",
    "\n",
    "# Time JAX (if available)\n",
    "time_jax = None\n",
    "if JAX_AVAILABLE and log_L_jax_instance:\n",
    "    time_jax = time_evaluations(log_L_jax_instance, theta_sample, num_warmup_calls, num_timed_calls, \"JAX\")\n",
    "\n",
    "if time_numpy and time_jax:\n",
    "    speedup = time_numpy / time_jax\n",
    "    print(f\"\\nJAX speed-up vs NumPy: {speedup:.2f}x\")\n",
    "elif time_numpy and not JAX_AVAILABLE:\n",
    "    print(\"\\nJAX benchmark not run as JAX is unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alternative: Using `scripts/bench_jax.py`\n",
    "\n",
    "For more robust and configurable benchmarking, you can use the dedicated script `scripts/bench_jax.py`. This script allows you to control the number of GW samples, host galaxies, evaluations, and warmup iterations. \n",
    "\n",
    "You can run it from your terminal (ensure you are in the repository root, or adjust the path to the script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Running the benchmark script from the notebook (if in 'examples/' directory)\n",
    "# Using ! to execute shell commands. Output will appear in the notebook.\n",
    "!python ../gw-siren-pipeline/scripts/bench_jax.py --num_hosts=500 --num_gw_samples=50 --num_evals=10 --num_warmup=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `bench_jax.py` will provide average timings for NumPy and JAX (if available on your system), along with the JAX device used (CPU/GPU/TPU) and the speed-up factor.\n",
    "\n",
    "**Note:** Significant speed-ups with JAX are typically observed with larger datasets (more host galaxies and GW samples) and when JAX can utilize GPU or TPU hardware. For small datasets on CPU, the overhead of JAX (including JIT compilation) might make it slower than or comparable to NumPy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
