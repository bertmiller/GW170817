import numpy as np
import sys
from numpy.polynomial.hermite import hermgauss
import emcee
import logging
import jax # Ensure jax is imported
from gwsiren import CONFIG
from gwsiren.backends import get_xp, logpdf_normal_xp, logsumexp_xp, trapz_xp
from gwsiren.distance_cache import create_distance_cache
from dataclasses import dataclass
import types # For types.ModuleType, if not already imported

logger = logging.getLogger(__name__)

# Module-level constants for memory management in NumPy backend path
MEMORY_THRESHOLD_BYTES = 4 * (1024**3)  # 4 GB
BYTES_PER_ELEMENT = 8  # for float64 (numpy.float64)

# Default Cosmological Parameters for Likelihood
DEFAULT_SIGMA_V_PEC = CONFIG.cosmology["sigma_v_pec"]  # km/s, peculiar velocity uncertainty
DEFAULT_C_LIGHT = CONFIG.cosmology["c_light"]
DEFAULT_OMEGA_M = CONFIG.cosmology["omega_m"]

# Default MCMC Parameters
DEFAULT_MCMC_N_DIM = 2
DEFAULT_MCMC_N_WALKERS = CONFIG.mcmc["walkers"]
DEFAULT_MCMC_N_STEPS = CONFIG.mcmc["steps"]
DEFAULT_MCMC_BURNIN = CONFIG.mcmc["burnin"]
DEFAULT_MCMC_THIN_BY = CONFIG.mcmc["thin_by"]
DEFAULT_MCMC_INITIAL_H0_MEAN = CONFIG.mcmc_initial_positions.h0_mean
DEFAULT_MCMC_INITIAL_H0_STD = CONFIG.mcmc_initial_positions.h0_std
DEFAULT_H0_PRIOR_MIN = CONFIG.mcmc["prior_h0_min"]  # km/s/Mpc
DEFAULT_H0_PRIOR_MAX = CONFIG.mcmc["prior_h0_max"]  # km/s/Mpc
DEFAULT_ALPHA_PRIOR_MIN = CONFIG.mcmc.get("prior_alpha_min", -1.0)
DEFAULT_ALPHA_PRIOR_MAX = CONFIG.mcmc.get("prior_alpha_max", 1.0)
DEFAULT_MCMC_INITIAL_ALPHA_MEAN = CONFIG.mcmc_initial_positions.alpha_mean
DEFAULT_MCMC_INITIAL_ALPHA_STD = CONFIG.mcmc_initial_positions.alpha_std

# Redshift marginalization parameters - now from config
DEFAULT_Z_ERR_THRESHOLD = CONFIG.redshift_marginalization.z_err_threshold
DEFAULT_QUAD_POINTS = CONFIG.redshift_marginalization.n_quad_points
DEFAULT_Z_MARGINALIZATION_SIGMA_RANGE = CONFIG.redshift_marginalization.sigma_range

# Batch processing for likelihood
DEFAULT_MCMC_BATCH_SIZE = CONFIG.mcmc.get("batch_size", 64)

# Static helper function for JIT compilation
def _static_comoving_integral_calculator_for_jit(xp_module, omega_m_val, z_scalar, N_trapz):
    """Computes integral(dz / E(z)) for a single scalar z_scalar using a specific xp_module."""
    
    # Define branches for jax.lax.cond
    def true_branch(_):
        return 0.0
    
    def false_branch(_):
        # z_steps must be generated by xp_module for trapz_xp
        z_steps = xp_module.linspace(0.0, z_scalar, N_trapz) 
        one_plus_z_steps = 1.0 + z_steps
        omega_m_val_f = xp_module.asarray(omega_m_val, dtype=xp_module.float64) # Ensure float for JAX
        
        Ez_sq = omega_m_val_f * (one_plus_z_steps**3) + (1.0 - omega_m_val_f)
        # Ensure Ez_sq is positive before sqrt, for numerical stability.
        Ez = xp_module.sqrt(xp_module.maximum(1e-18, Ez_sq)) 
        
        # Avoid division by zero if any Ez element is zero
        integrand = 1.0 / xp_module.maximum(1e-18, Ez)
        
        # Use backend-agnostic trapz
        integral = trapz_xp(xp_module, integrand, x=z_steps)
        return integral

    # Use jax.lax.cond for the conditional check
    # The condition z_scalar < 1e-9 might still cause issues if xp_module is not jax.numpy
    # when this function is JITted. However, _jitted_static_comoving_integral is called only when backend is JAX.
    # For JAX, z_scalar will be a JAX array/tracer.
    return jax.lax.cond(z_scalar < 1e-9, true_branch, false_branch, None)

_jitted_static_comoving_integral = jax.jit(
    _static_comoving_integral_calculator_for_jit, 
    static_argnames=('xp_module', 'N_trapz')
)

# Static helper function for JIT compilation of lum_dist_model_uncached logic
def _static_lum_dist_calculator_for_jit(xp_module, c_val, omega_m_val, H0_val, z_values_backend_array, N_trapz, _scalar_integral_fn_to_call):
    """Core logic for luminosity distance calculation for JIT.
    Assumes H0_val is valid and z_values_backend_array is already a backend array.
    """
    dc_integrals_list = [_scalar_integral_fn_to_call(xp_module, omega_m_val, z_val, N_trapz) for z_val in z_values_backend_array]
    dc_integrals = xp_module.asarray(dc_integrals_list, dtype=xp_module.float64)
    lum_dist = (c_val / H0_val) * (1.0 + z_values_backend_array) * dc_integrals
    return lum_dist

_jitted_static_lum_dist = jax.jit(
    _static_lum_dist_calculator_for_jit,
    static_argnames=('xp_module', 'c_val', 'omega_m_val', 'N_trapz', '_scalar_integral_fn_to_call')
)

def _core_static_marginalize_one_galaxy_jax(
    xp_module: types.ModuleType,
    H0_val_jnp: jax.numpy.ndarray,
    mu_z_scalar_jnp: jax.numpy.ndarray,
    sigma_z_scalar_jnp: jax.numpy.ndarray,
    dL_gw_samples_jnp: jax.numpy.ndarray,
    _quad_nodes_jnp: jax.numpy.ndarray,
    _quad_weights_jnp: jax.numpy.ndarray,
    c_val: float,
    sigma_v_val: float,
    omega_m_val: float, 
    _jitted_lum_dist_calculator_func: callable,
    _jitted_comoving_integral_func: callable,
    z_sigma_range: float, # Unused in this direct implementation, but kept for signature consistency if needed later
    N_trapz_lum_dist: int
) -> jax.numpy.ndarray:
    """
    Performs redshift marginalization for a single galaxy using JAX logic.
    This is a pure, static function not intended for direct JIT compilation itself in this step.
    It uses a pre-jitted luminosity distance calculator.
    """
    # Ensure backend functions are accessible if they were imported conditionally or aliased
    # For example, if logpdf_normal_xp and logsumexp_xp are module-level in backends.py
    # and backends.py is imported as `backends_module`, then:
    # logpdf_normal_xp_func = backends_module.logpdf_normal_xp
    # logsumexp_xp_func = backends_module.logsumexp_xp
    # For simplicity, assuming direct availability based on typical import patterns in the file.
    # If this function is moved or backends.py has complex aliasing, this might need adjustment.
    
    # Transform quadrature nodes: z_j = mu_z + sqrt(2) * sigma_z * x_i
    # x_i are the _quad_nodes_jnp
    z_j_array = mu_z_scalar_jnp + xp_module.sqrt(2.0) * sigma_z_scalar_jnp * _quad_nodes_jnp
    
    # Filter out non-positive z_j values
    valid_z_mask_for_quad = z_j_array > 1e-6 # Small epsilon to avoid issues at exactly zero
    
    # Create z_j for distance calculation, replacing invalid points with 0 (or another suitable placeholder)
    # The lum_dist calculator should handle z=0 appropriately (e.g., returning 0 distance or handling the integral).
    z_j_for_dist_calc = xp_module.where(valid_z_mask_for_quad, z_j_array, 0.0)

    # Call the JITted luminosity distance function
    # _jitted_static_lum_dist expects: 
    # (xp_module, c_val, omega_m_val, H0_val, z_values_backend_array, N_trapz, _scalar_integral_fn_to_call)
    # Here, z_values_backend_array is z_j_for_dist_calc (shape (N_quad_points,))
    # H0_val is H0_val_jnp (scalar)
    model_d_at_quad_nodes = _jitted_lum_dist_calculator_func(
        xp_module, 
        c_val, 
        omega_m_val, 
        H0_val_jnp, 
        z_j_for_dist_calc, 
        N_trapz_lum_dist, 
        _jitted_comoving_integral_func
    )

    # Create a mask for valid distances (finite and positive)
    valid_d_mask = xp_module.isfinite(model_d_at_quad_nodes) & (model_d_at_quad_nodes > 0.0)
    
    # Combine masks: points are overall valid if their redshift and resulting distance are valid
    overall_valid_points_mask = valid_z_mask_for_quad & valid_d_mask
    
    # Calculate sigma_d (uncertainty in dL_model due to peculiar velocities)
    # sigma_d = (dL_model / c) * sigma_v
    # Add a small epsilon to avoid division by zero if model_d_at_quad_nodes is zero,
    # or if sigma_d becomes exactly zero, which can be problematic for logpdf_normal.
    sigma_d_at_quad_nodes = xp_module.maximum((model_d_at_quad_nodes / c_val) * sigma_v_val, 1e-9)

    # Expand dimensions for broadcasting in logpdf_normal_xp
    # dL_gw_samples_jnp: (N_gw_samples,) -> (N_gw_samples, 1)
    # model_d_at_quad_nodes: (N_quad_points,) -> (1, N_quad_points)
    # sigma_d_at_quad_nodes: (N_quad_points,) -> (1, N_quad_points)
    dL_gw_samples_expanded = dL_gw_samples_jnp[:, xp_module.newaxis]
    model_d_expanded = model_d_at_quad_nodes[xp_module.newaxis, :]
    sigma_d_expanded = sigma_d_at_quad_nodes[xp_module.newaxis, :]
    
    # Calculate log PDF values: log p(dL_gw | dL_model(z_j, H0), sigma_d(z_j, H0))
    # logpdf_normal_xp(xp_module, x, loc, scale)
    log_pdf_values = logpdf_normal_xp(xp_module, dL_gw_samples_expanded, loc=model_d_expanded, scale=sigma_d_expanded)
    # log_pdf_values shape: (N_gw_samples, N_quad_points)

    # Handle NaNs in log_pdf_values by replacing them with -inf
    log_pdf_values = xp_module.where(~xp_module.isfinite(log_pdf_values), -xp_module.inf, log_pdf_values)
    
    # Average likelihood over GW samples (marginalize over dL_gw samples)
    # log L(H0 | z_j) = log [ (1/N_gw) * sum_k p(dL_gw_k | z_j, H0) ]
    #                = logsumexp_k [ log p(dL_gw_k | z_j, H0) ] - log(N_gw)
    num_gw_samples = xp_module.array(len(dL_gw_samples_jnp), dtype=xp_module.float64) # Ensure float for log
    log_likelihood_at_quad_nodes = logsumexp_xp(xp_module, log_pdf_values, axis=0) - xp_module.log(num_gw_samples)
    # log_likelihood_at_quad_nodes shape: (N_quad_points,)

    # Apply the overall validity mask to the log-likelihoods at each quadrature node
    # Set log-likelihood to -inf for invalid points
    log_likelihood_at_quad_nodes = xp_module.where(
        overall_valid_points_mask, 
        log_likelihood_at_quad_nodes, 
        -xp_module.inf
    )

    # Prepare the integrand for Gaussian quadrature
    # Integrand = L(H0 | z_j) * p(z_j | mu_z, sigma_z)
    # In log space, for Gauss-Hermite:
    # log_integrand = log L(H0 | z_j) + log(w_i / sqrt(pi))
    # where w_i are the _quad_weights_jnp
    log_integrand = log_likelihood_at_quad_nodes + xp_module.log(_quad_weights_jnp / xp_module.sqrt(xp_module.pi))
    
    # Perform the final marginalization over redshift using logsumexp for numerical stability
    # log L_gal(H0) = log sum_j [ exp(log_integrand_j) ]
    #               = logsumexp_j [ log_integrand_j ]
    marginalized_log_L = logsumexp_xp(xp_module, log_integrand, axis=0) # axis=0 sums over N_quad_points
    
    return marginalized_log_L

# JIT-compiled version of the core static marginalization function
_jitted_core_static_marginalize_one_galaxy = jax.jit(
    _core_static_marginalize_one_galaxy_jax,
    static_argnames=(
        'xp_module', 
        'c_val', 
        'sigma_v_val', 
        'omega_m_val',
        '_jitted_lum_dist_calculator_func',
        '_jitted_comoving_integral_func',
        'z_sigma_range',
        'N_trapz_lum_dist'
    )
)

class H0LogLikelihood:
    """Log-likelihood for joint inference of ``H0`` and ``alpha``.

    Args:
        dL_gw_samples: Array of GW luminosity distance samples.
        host_galaxies_z: Redshifts of candidate host galaxies.
        host_galaxies_mass_proxy: Positive mass proxy values for the galaxies.
        host_galaxies_z_err: Redshift uncertainties for the galaxies.
        sigma_v: Peculiar velocity dispersion in km/s.
        c_val: Speed of light in km/s.
        omega_m_val: Matter density parameter.
        h0_min: Lower prior bound on ``H0``.
        h0_max: Upper prior bound on ``H0``.
        alpha_min: Lower prior bound on ``alpha``.
        alpha_max: Upper prior bound on ``alpha``.
        use_vectorized_likelihood: Whether to use the vectorised likelihood
            implementation.
    """
    def __init__(
        self,
        xp,
        backend_name,
        dL_gw_samples,
        host_galaxies_z,
        host_galaxies_mass_proxy,
        host_galaxies_z_err,
        sigma_v=DEFAULT_SIGMA_V_PEC,
        c_val=DEFAULT_C_LIGHT,
        omega_m_val=DEFAULT_OMEGA_M,
        h0_min=DEFAULT_H0_PRIOR_MIN,
        h0_max=DEFAULT_H0_PRIOR_MAX,
        alpha_min=DEFAULT_ALPHA_PRIOR_MIN,
        alpha_max=DEFAULT_ALPHA_PRIOR_MAX,
        use_vectorized_likelihood=False,
        z_err_threshold=DEFAULT_Z_ERR_THRESHOLD,
        n_quad_points=DEFAULT_QUAD_POINTS,
        z_sigma_range=DEFAULT_Z_MARGINALIZATION_SIGMA_RANGE,
        batch_size=DEFAULT_MCMC_BATCH_SIZE,
    ):
        """Initialize H0LogLikelihood with backend-agnostic computation setup.
        
        Args:
            xp: Backend computation module
            backend_name: Name of the backend
            dL_gw_samples: GW distance samples
            host_galaxies_z: Host redshifts  
            host_galaxies_mass_proxy: Mass proxy values
            host_galaxies_z_err: Redshift uncertainties
            ... (other parameters as before)
        """
        # 1. Store backend configuration
        self._setup_backend_config(xp, backend_name)
        
        # 2. Validate and process input data
        self._setup_input_data(
            dL_gw_samples, host_galaxies_z, host_galaxies_mass_proxy, host_galaxies_z_err
        )
        
        # 3. Store computational parameters
        self._setup_computational_parameters(
            sigma_v, c_val, omega_m_val, h0_min, h0_max, alpha_min, alpha_max,
            use_vectorized_likelihood, z_err_threshold, n_quad_points, z_sigma_range,
            batch_size
        )
        
        # 4. Initialize quadrature integration
        self._setup_quadrature_integration(n_quad_points)
        
        # 5. Apply backend-specific optimizations
        self._apply_backend_optimizations()

    def _setup_backend_config(self, xp, backend_name):
        """Store backend configuration for computation.
        
        Args:
            xp: Backend computation module
            backend_name: Name of the backend
        """
        self.xp = xp
        self.backend_name = backend_name

    def _setup_input_data(self, dL_gw_samples, host_galaxies_z, host_galaxies_mass_proxy, host_galaxies_z_err):
        """Validate and convert input data to backend arrays.
        
        Args:
            dL_gw_samples: GW distance samples
            host_galaxies_z: Host redshifts  
            host_galaxies_mass_proxy: Mass proxy values
            host_galaxies_z_err: Redshift uncertainties
        """
        # Validate inputs using NumPy for robustness
        self._validate_input_data(dL_gw_samples, host_galaxies_z, host_galaxies_mass_proxy, host_galaxies_z_err)
        
        # Convert to backend arrays
        self.dL_gw_samples = self.xp.asarray(dL_gw_samples)
        
        # Process galaxy data with proper dimensionality handling
        self.z_values, self.mass_proxy_values, self.z_err_values = self._process_galaxy_data(
            host_galaxies_z, host_galaxies_mass_proxy, host_galaxies_z_err
        )
        
        # Validate data consistency
        self._validate_data_consistency()

    def _validate_input_data(self, dL_gw_samples, host_galaxies_z, host_galaxies_mass_proxy, host_galaxies_z_err):
        """Validate input data before processing.
        
        Args:
            dL_gw_samples: GW distance samples
            host_galaxies_z: Host redshifts
            host_galaxies_mass_proxy: Mass proxy values
            host_galaxies_z_err: Redshift uncertainties
            
        Raises:
            ValueError: If any input is invalid
        """
        if dL_gw_samples is None or len(np.asarray(dL_gw_samples)) == 0:
            raise ValueError("dL_gw_samples cannot be None or empty.")
        if host_galaxies_z is None or len(np.asarray(host_galaxies_z)) == 0:
            raise ValueError("host_galaxies_z cannot be None or empty.")
        if host_galaxies_mass_proxy is None or len(np.asarray(host_galaxies_mass_proxy)) == 0:
            raise ValueError("host_galaxies_mass_proxy cannot be None or empty.")
        if host_galaxies_z_err is None or len(np.asarray(host_galaxies_z_err)) == 0:
            raise ValueError("host_galaxies_z_err cannot be None or empty.")

    def _process_galaxy_data(self, host_galaxies_z, host_galaxies_mass_proxy, host_galaxies_z_err):
        """Process galaxy data ensuring proper dimensionality.
        
        Args:
            host_galaxies_z: Host redshifts
            host_galaxies_mass_proxy: Mass proxy values  
            host_galaxies_z_err: Redshift uncertainties
            
        Returns:
            tuple: (z_values, mass_proxy_values, z_err_values) as backend arrays
        """
        # Convert to NumPy first for reliable shape handling
        z_values_np = np.asarray(host_galaxies_z, dtype=float)
        mass_proxy_values_np = np.asarray(host_galaxies_mass_proxy, dtype=float)
        z_err_values_np = np.asarray(host_galaxies_z_err, dtype=float)

        # Ensure at least 1D arrays
        if z_values_np.ndim == 0:
            z_values_np = np.array([z_values_np])
        if mass_proxy_values_np.ndim == 0:
            mass_proxy_values_np = np.array([mass_proxy_values_np])
        if z_err_values_np.ndim == 0:
            z_err_values_np = np.array([z_err_values_np])
        
        # Convert to backend arrays
        z_values = self.xp.asarray(z_values_np)
        mass_proxy_values = self.xp.asarray(mass_proxy_values_np)
        z_err_values = self.xp.asarray(z_err_values_np)
        
        return z_values, mass_proxy_values, z_err_values

    def _validate_data_consistency(self):
        """Validate that all galaxy data arrays have consistent lengths.
        
        Raises:
            ValueError: If array lengths are inconsistent
        """
        if len(self.mass_proxy_values) != len(self.z_values):
            raise ValueError("host_galaxies_mass_proxy and host_galaxies_z must have the same length")
        if len(self.z_err_values) != len(self.z_values):
            raise ValueError("host_galaxies_z_err and host_galaxies_z must have the same length")

    def _setup_computational_parameters(self, sigma_v, c_val, omega_m_val, h0_min, h0_max, 
                                      alpha_min, alpha_max, use_vectorized_likelihood,
                                      z_err_threshold, n_quad_points, z_sigma_range,
                                      batch_size):
        """Store computational parameters for likelihood evaluation.
        
        Args:
            sigma_v: Peculiar velocity dispersion
            c_val: Speed of light
            omega_m_val: Matter density parameter
            h0_min, h0_max: H0 prior bounds
            alpha_min, alpha_max: Alpha prior bounds
            use_vectorized_likelihood: Vectorization flag
            z_err_threshold: Redshift error threshold for marginalization
            n_quad_points: Number of quadrature points
            z_sigma_range: Range for redshift integration
        """
        # Physical parameters
        self.sigma_v = sigma_v
        self.c_val = c_val
        self.omega_m_val = omega_m_val
        
        # Prior bounds
        self.h0_min = h0_min
        self.h0_max = h0_max
        self.alpha_min = alpha_min
        self.alpha_max = alpha_max
        
        # Computational settings
        self.use_vectorized_likelihood = use_vectorized_likelihood
        self.z_err_threshold = z_err_threshold
        self.n_quad_points = n_quad_points
        self.z_sigma_range = z_sigma_range
        self.batch_size = batch_size

    def _setup_quadrature_integration(self, n_quad_points):
        """Initialize Gaussian quadrature nodes and weights.
        
        Args:
            n_quad_points: Number of quadrature points
        """
        # Generate quadrature nodes and weights using NumPy
        quad_nodes_np, quad_weights_np = hermgauss(n_quad_points)
        
        # Convert to backend arrays
        self._quad_nodes = self.xp.asarray(quad_nodes_np)
        self._quad_weights = self.xp.asarray(quad_weights_np)

    def _apply_backend_optimizations(self):
        """Apply backend-specific optimizations like JIT compilation.
        
        For JAX backend, this applies JIT compilation to the __call__ method.
        For NumPy backend, this is a no-op.
        """
        # Initialize distance cache for performance optimization
        self._distance_cache = create_distance_cache(
            max_cache_size=100,  # Cache up to 100 H0 values
            z_range=(0.001, 3.0),  # Reasonable redshift range
            z_points=2000,  # High resolution for accuracy
            h0_tolerance=0.01  # Group H0 values within 0.01 km/s/Mpc
        )
        
        # Set up the uncached distance function for the cache to use
        self._distance_cache.set_distance_function(self._lum_dist_model_uncached)
        
        if self.backend_name == "jax":
            self._apply_jax_optimizations()

    def _apply_jax_optimizations(self):
        """Apply JAX-specific optimizations like JIT compilation."""
        if self.backend_name == "jax":
            # For JAX backend, we can JIT compile the main computation
            # This is a placeholder for future JAX-specific optimizations
            pass
    
    def _lum_dist_model_uncached(self, z_values, H0_val, N_trapz=200):
        """Uncached version of luminosity distance computation.
        
        This is the original computation that the cache will use to build
        interpolation tables. Should not be called directly - use _lum_dist_model instead.
        """
        # Ensure z_values is an array and determine if original input was scalar
        z_input_asarray = self.xp.asarray(z_values)
        is_scalar_input = (z_input_asarray.ndim == 0)
        z_input_arr = self.xp.atleast_1d(z_input_asarray) # Work with at least 1D array

        if H0_val < 1e-9: # Avoid division by zero for H0
            return self.xp.full_like(z_input_arr, self.xp.inf, dtype=self.xp.float64)[0] if is_scalar_input else self.xp.full_like(z_input_arr, self.xp.inf, dtype=self.xp.float64)

        if self.backend_name == "jax":
            # Call the JITted static function for JAX backend
            result_arr = _jitted_static_lum_dist(
                self.xp, 
                self.c_val, 
                self.omega_m_val, 
                H0_val, 
                z_input_arr, 
                N_trapz, 
                _jitted_static_comoving_integral # Pass the JITted scalar integral function
            )
        else:
            # Original logic for NumPy backend (or other non-JAX backends)
            dc_integrals_list = [self._scalar_comoving_distance_integral(z_val, N_trapz) for z_val in z_input_arr]
            dc_integrals = self.xp.asarray(dc_integrals_list, dtype=self.xp.float64)
            result_arr = (self.c_val / H0_val) * (1.0 + z_input_arr) * dc_integrals
        
        # Handle scalar input: if original z_values was scalar, return a scalar
        if is_scalar_input:
            return result_arr[0] 
        return result_arr

    def _lum_dist_model(self, z_values, H0_val, N_trapz=200):
        """Compute luminosity distance for z_values (array or scalar) and H0_val.
        
        Uses distance cache for performance optimization. This replaces the original
        distance computation with a cached version that builds interpolation tables.
        """
        # Use the distance cache for optimized computation
        result = self._distance_cache.get_distances(z_values, H0_val)
        
        # Convert result to backend array if needed
        # The distance cache returns NumPy arrays/scalars, we need to convert to backend arrays
        if self.backend_name == 'jax':
            # For JAX, always convert to JAX arrays
            if self.xp.isscalar(z_values):
                return self.xp.array(result)
            else:
                return self.xp.asarray(result)
        else:
            # For NumPy, check if conversion is needed
            if hasattr(result, '__array__') and hasattr(result, 'dtype'):
                # Already an array-like object
                return self.xp.asarray(result)
            else:
                # Scalar or other type
                if self.xp.isscalar(z_values):
                    return self.xp.array(result)
                else:
                    return self.xp.asarray(result)

    def _scalar_comoving_distance_integral(self, z_scalar, N_trapz=200):
        """Computes integral(dz / E(z)) for a single scalar z_scalar using backend-agnostic trapz."""
        if self.backend_name == "jax":
            return _jitted_static_comoving_integral(self.xp, self.omega_m_val, z_scalar, N_trapz)
        else:
            if z_scalar < 1e-9: # Threshold for being effectively zero
                return 0.0
            
            # z_steps must be generated by self.xp for trapz_xp
            z_steps = self.xp.linspace(0.0, z_scalar, N_trapz) 
            one_plus_z_steps = 1.0 + z_steps
            omega_m_val_f = self.xp.asarray(self.omega_m_val, dtype=self.xp.float64) # Ensure float for JAX
            
            Ez_sq = omega_m_val_f * (one_plus_z_steps**3) + (1.0 - omega_m_val_f)
            # Ensure Ez_sq is positive before sqrt, for numerical stability.
            Ez = self.xp.sqrt(self.xp.maximum(1e-18, Ez_sq)) 
            
            # Avoid division by zero if any Ez element is zero
            integrand = 1.0 / self.xp.maximum(1e-18, Ez)
            
            # Use backend-agnostic trapz instead of self.xp.trapz
            integral = trapz_xp(self.xp, integrand, x=z_steps)
            return integral

    def __call__(self, theta):
        """Compute log-likelihood for H0 and alpha parameters."""
        H0, alpha = theta[0], theta[1]
        
        # 1. Validate parameters
        if not self._validate_parameters(H0, alpha):
            return -self.xp.inf
            
        # 2. Compute distances and uncertainties  
        distances_result = self._compute_distances_and_uncertainties(H0)
        if distances_result is None:
            return -self.xp.inf
        model_distances, distance_uncertainties = distances_result
            
        # 3. Compute likelihood contributions from all galaxies
        galaxy_log_likelihoods = self._compute_galaxy_likelihoods(
            model_distances, distance_uncertainties, H0
        )
        if galaxy_log_likelihoods is None:
            return -self.xp.inf
            
        # 4. Apply mass weighting based on alpha
        galaxy_weights = self._compute_galaxy_weights(alpha)  
        if galaxy_weights is None:
            return -self.xp.inf
            
        # 5. Combine weighted likelihoods
        return self._combine_weighted_likelihoods(galaxy_log_likelihoods, galaxy_weights)

    def _validate_parameters(self, H0, alpha):
        """Validate H0 and alpha are within their respective prior bounds.
        
        Args:
            H0 (float): Hubble constant value to validate
            alpha (float): Mass bias parameter to validate
            
        Returns:
            bool: True if both parameters are within bounds, False otherwise
        """
        h0_valid = self.h0_min <= H0 <= self.h0_max
        alpha_valid = self.alpha_min <= alpha <= self.alpha_max
        return h0_valid and alpha_valid
        
    def _compute_distances_and_uncertainties(self, H0):
        """Compute model luminosity distances and their uncertainties.
        
        Args:
            H0 (float): Hubble constant value
            
        Returns:
            tuple: (model_distances, distance_uncertainties) or None if computation fails
        """
        try:
            # Compute luminosity distances for all host galaxies
            model_d_for_hosts = self._lum_dist_model(self.z_values, H0)
            
            # Check for finite values
            if self.xp.any(~self.xp.isfinite(model_d_for_hosts)):
                return None
                
            # Compute distance uncertainties from peculiar velocity
            sigma_d_val_for_hosts = (model_d_for_hosts / self.c_val) * self.sigma_v
            sigma_d_val_for_hosts = self.xp.maximum(sigma_d_val_for_hosts, 1e-9)
            
            # Check for finite uncertainties
            if self.xp.any(~self.xp.isfinite(sigma_d_val_for_hosts)):
                return None
                
            return model_d_for_hosts, sigma_d_val_for_hosts
            
        except Exception:
            return None

    def _compute_galaxy_likelihoods(self, model_distances, distance_uncertainties, H0):
        """Compute likelihood contributions from all galaxies.
        
        Dispatches to either vectorized or looped implementation based on 
        the use_vectorized_likelihood setting.
        
        Args:
            model_distances: Luminosity distances for each galaxy
            distance_uncertainties: Distance uncertainties for each galaxy  
            H0 (float): Hubble constant value
            
        Returns:
            array: Log-likelihood contribution for each galaxy, or None if computation fails
        """
        if self.use_vectorized_likelihood:
            return self._compute_galaxy_likelihoods_vectorized(
                model_distances, distance_uncertainties, H0
            )
        else:
            return self._compute_galaxy_likelihoods_looped(
                model_distances, distance_uncertainties, H0
            )

    def _compute_galaxy_likelihoods_vectorized(self, model_distances, distance_uncertainties, H0):
        """Compute galaxy likelihoods using vectorized operations for memory efficiency.
        
        Args:
            model_distances: Luminosity distances for each galaxy
            distance_uncertainties: Distance uncertainties for each galaxy
            H0 (float): Hubble constant value
            
        Returns:
            array: Log-likelihood contribution for each galaxy, or None if computation fails
        """
        # Check which galaxies need redshift marginalization
        needs_marginalization = self.z_err_values >= self.z_err_threshold
        
        if not self.xp.any(needs_marginalization):
            # All galaxies below threshold - use simple vectorized computation
            return self._compute_simple_vectorized_likelihoods(
                model_distances, distance_uncertainties
            )
        else:
            # Use new batched approach if not using full vectorization
            return self._compute_galaxy_likelihoods_batched(
                model_distances, distance_uncertainties, H0
            )

    def _compute_simple_vectorized_likelihoods(self, model_distances, distance_uncertainties):
        """Compute likelihoods when no redshift marginalization is needed.
        
        Args:
            model_distances: Luminosity distances for each galaxy
            distance_uncertainties: Distance uncertainties for each galaxy
            
        Returns:
            array: Log-likelihood contribution for each galaxy, or None if computation fails
        """
        try:
            # Ensure arrays are properly shaped for broadcasting
            # Convert to backend arrays and ensure they're at least 1D
            model_distances = self.xp.atleast_1d(self.xp.asarray(model_distances))
            distance_uncertainties = self.xp.atleast_1d(self.xp.asarray(distance_uncertainties))
            dL_gw_samples = self.xp.atleast_1d(self.xp.asarray(self.dL_gw_samples))
            
            # Create properly shaped arrays for broadcasting
            # Shape: (N_samples, 1) and (1, N_hosts)
            gw_samples_expanded = dL_gw_samples[:, self.xp.newaxis]
            model_distances_expanded = model_distances[self.xp.newaxis, :]
            distance_uncertainties_expanded = distance_uncertainties[self.xp.newaxis, :]
            
            log_pdf_values_full = logpdf_normal_xp(
                self.xp,
                gw_samples_expanded,
                loc=model_distances_expanded,
                scale=distance_uncertainties_expanded
            )  # Shape: (N_samples, N_hosts)
        except Exception:
            return None

        # Handle potential NaNs by converting them to -inf
        log_pdf_values_full = self.xp.where(
            self.xp.isnan(log_pdf_values_full), -self.xp.inf, log_pdf_values_full
        )

        # Sum over GW samples and normalize
        log_sum_over_gw_samples = logsumexp_xp(self.xp, log_pdf_values_full, axis=0) - \
                                 self.xp.log(self.xp.array(len(self.dL_gw_samples), dtype=self.xp.float64))
        
        return log_sum_over_gw_samples

    def _compute_galaxy_likelihoods_batched(self, model_distances, distance_uncertainties, H0):
        """Compute galaxy likelihoods using a batched and vectorized approach.
        
        Args:
            model_distances: Luminosity distances for each galaxy (N_total_hosts,)
            distance_uncertainties: Distance uncertainties for each galaxy (N_total_hosts,)
            H0 (float): Hubble constant value
            
        Returns:
            array: Log-likelihood contribution for each galaxy, or None if computation fails
        """
        n_total_hosts = len(self.z_values)
        # Initialize with a value indicating failure or uncomputed, like -inf
        log_P_data_H0_zi_terms = self.xp.full(n_total_hosts, -self.xp.inf)

        for i_batch_start in range(0, n_total_hosts, self.batch_size):
            i_batch_end = min(i_batch_start + self.batch_size, n_total_hosts)
            
            current_batch_size = i_batch_end - i_batch_start
            if current_batch_size == 0:
                continue

            # Slice data for the current batch
            # Ensure slices are used to create new arrays for JAX compatibility if modified later
            # or if they are intermediate results that JAX might trace.
            z_batch = self.xp.asarray(self.z_values[i_batch_start:i_batch_end])
            z_err_batch = self.xp.asarray(self.z_err_values[i_batch_start:i_batch_end])
            model_distances_batch_slice = self.xp.asarray(model_distances[i_batch_start:i_batch_end])
            distance_uncertainties_batch_slice = self.xp.asarray(distance_uncertainties[i_batch_start:i_batch_end])
            
            # Determine which galaxies in the batch need marginalization
            needs_marginalization_mask_in_batch = z_err_batch >= self.z_err_threshold
            no_marginalization_mask_in_batch = ~needs_marginalization_mask_in_batch

            # Temporary array for this batch's results, initialized to -inf
            batch_log_likelihoods = self.xp.full(current_batch_size, -self.xp.inf)

            # --- Process galaxies in batch that DO NOT need marginalization ---
            if self.xp.any(no_marginalization_mask_in_batch):
                # Convert boolean mask to integer mask for indexing if needed by backend or for sum
                num_no_marg_in_batch = self.xp.sum(no_marginalization_mask_in_batch.astype(self.xp.int32))
                if num_no_marg_in_batch > 0:
                    # Select data for non-marginalized galaxies in the batch
                    model_dist_no_marg_batch = model_distances_batch_slice[no_marginalization_mask_in_batch]
                    dist_unc_no_marg_batch = distance_uncertainties_batch_slice[no_marginalization_mask_in_batch]
                    
                    # Expand dimensions for broadcasting with GW samples
                    # gw_samples_expanded shape: (N_gw_samples, 1)
                    # model_dist_expanded shape: (1, num_no_marg_in_batch)
                    # dist_unc_expanded shape: (1, num_no_marg_in_batch)
                    gw_samples_expanded = self.dL_gw_samples[:, self.xp.newaxis]
                    model_dist_expanded = model_dist_no_marg_batch[self.xp.newaxis, :]
                    dist_unc_expanded = dist_unc_no_marg_batch[self.xp.newaxis, :]
                    
                    log_pdf_values_no_marg = logpdf_normal_xp(
                        self.xp, gw_samples_expanded, loc=model_dist_expanded, scale=dist_unc_expanded
                    ) # Shape: (N_gw_samples, num_no_marg_in_batch)

                    log_pdf_values_no_marg = self.xp.where(
                        self.xp.isnan(log_pdf_values_no_marg), -self.xp.inf, log_pdf_values_no_marg
                    )
                    log_sum_no_marg_results = (logsumexp_xp(self.xp, log_pdf_values_no_marg, axis=0) -
                                             self.xp.log(self.xp.array(len(self.dL_gw_samples), dtype=self.xp.float64)))
                    
                    # Update batch_log_likelihoods for non-marginalized galaxies
                    if self.backend_name == "jax":
                        # JAX requires explicit indexing for updates
                        true_indices_no_marg_in_batch = self.xp.where(no_marginalization_mask_in_batch)[0]
                        batch_log_likelihoods = batch_log_likelihoods.at[true_indices_no_marg_in_batch].set(log_sum_no_marg_results)
                    else:
                        # NumPy allows direct boolean mask assignment
                        batch_log_likelihoods[no_marginalization_mask_in_batch] = log_sum_no_marg_results
            
            # --- Process galaxies in batch that DO need marginalization ---
            if self.xp.any(needs_marginalization_mask_in_batch):
                num_marg_in_batch = self.xp.sum(needs_marginalization_mask_in_batch.astype(self.xp.int32))
                if num_marg_in_batch > 0:
                    # Select data for marginalized galaxies in the batch
                    z_marg_batch_data = z_batch[needs_marginalization_mask_in_batch]
                    z_err_marg_batch_data = z_err_batch[needs_marginalization_mask_in_batch]

                    # Call the new batched marginalization function
                    log_sum_marg_results = self._marginalize_batch_galaxy_redshift(
                        H0, z_marg_batch_data, z_err_marg_batch_data
                    )
                    
                    # Update batch_log_likelihoods for marginalized galaxies
                    if self.backend_name == "jax":
                        true_indices_marg_in_batch = self.xp.where(needs_marginalization_mask_in_batch)[0]
                        batch_log_likelihoods = batch_log_likelihoods.at[true_indices_marg_in_batch].set(log_sum_marg_results)
                    else:
                        batch_log_likelihoods[needs_marginalization_mask_in_batch] = log_sum_marg_results
                        
            # --- Store batch results in the main array ---
            if self.backend_name == "jax":
                # For JAX, use .at[].set() with an array of indices
                batch_indices_for_jax = self.xp.arange(i_batch_start, i_batch_end, dtype=self.xp.int32)
                log_P_data_H0_zi_terms = log_P_data_H0_zi_terms.at[batch_indices_for_jax].set(batch_log_likelihoods)
            else:
                # NumPy allows direct slice assignment
                log_P_data_H0_zi_terms[i_batch_start:i_batch_end] = batch_log_likelihoods
        
        # Final check for NaNs or non-finite values that might have slipped through
        log_P_data_H0_zi_terms = self.xp.where(
            self.xp.isnan(log_P_data_H0_zi_terms), -self.xp.inf, log_P_data_H0_zi_terms
        )
        log_P_data_H0_zi_terms = self.xp.where(
            ~self.xp.isfinite(log_P_data_H0_zi_terms), -self.xp.inf, log_P_data_H0_zi_terms
        )
            
        return log_P_data_H0_zi_terms

    def _marginalize_batch_galaxy_redshift(self, H0, z_batch_marg, z_err_batch_marg):
        """Perform redshift marginalization for a batch of galaxies using vectorized quadrature.
        
        Args:
            H0 (float): Hubble constant value
            z_batch_marg: Mean redshifts for the batch of galaxies needing marginalization (N_batch_marg,)
            z_err_batch_marg: Redshift uncertainties for the batch (N_batch_marg,)
            
        Returns:
            array: Marginalized log-likelihoods for this batch of galaxies (N_batch_marg,)
        """
        N_batch_marg = len(z_batch_marg)
        if N_batch_marg == 0:
            # Return an empty array of the correct type if the batch is empty
            return self.xp.array([], dtype=self.xp.float64)

        # 1. Transform quadrature nodes to redshift domain for the batch
        # self._quad_nodes shape: (N_quad_points,)
        # z_batch_marg[:, self.xp.newaxis] shape: (N_batch_marg, 1)
        # z_quad_batch shape: (N_batch_marg, N_quad_points)
        z_quad_batch = (z_batch_marg[:, self.xp.newaxis] +
                       self.xp.sqrt(2.0) * z_err_batch_marg[:, self.xp.newaxis] *
                       self._quad_nodes[self.xp.newaxis, :])

        # 2. Filter out negative redshifts and handle validity per galaxy
        # valid_z_mask_batch shape: (N_batch_marg, N_quad_points)
        # Using 1e-6 as a threshold, consistent with _scalar_comoving_distance_integral and other places
        valid_z_mask_batch = z_quad_batch > 1e-6 

        # Create a version of z_quad_batch for distance calculation.
        # Replace non-valid z with a placeholder (e.g., 0.0 or a small positive value).
        # The behavior of _lum_dist_model with these placeholders needs to be robust
        # (e.g., return self.xp.inf or a very large distance that leads to ~0 likelihood).
        # For now, using 0.0; _lum_dist_model handles z near zero.
        z_quad_for_dist_calc = self.xp.where(valid_z_mask_batch, z_quad_batch, 0.0)

        # 3. Vectorized distance computation for all (galaxy, quadrature_point) combinations
        # model_d_quad_batch output shape expected: (N_batch_marg, N_quad_points)
        # Note: _lum_dist_model needs to handle a 2D array of z_values correctly.
        # If it expects 1D, this might need adjustment or a wrapper.
        # Assuming _lum_dist_model can take (N_batch_marg, N_quad_points) and return same shape.
        model_d_quad_batch = self._lum_dist_model(z_quad_for_dist_calc.flatten(), H0)
        model_d_quad_batch = model_d_quad_batch.reshape(N_batch_marg, self.n_quad_points)
        
        # Check for valid distances (finite and positive)
        valid_d_mask_batch = self.xp.isfinite(model_d_quad_batch) & (model_d_quad_batch > 0)
        
        # Combine validity masks: a point is valid if its redshift and derived distance are valid.
        # overall_valid_points_mask shape: (N_batch_marg, N_quad_points)
        overall_valid_points_mask = valid_z_mask_batch & valid_d_mask_batch

        # 4. Compute sigma_d for each (galaxy, quadrature_point)
        # sigma_d_quad_batch shape: (N_batch_marg, N_quad_points)
        # Use model_d_quad_batch; if it was inf/nan, sigma_d will also be, and logpdf handles it.
        sigma_d_quad_batch = self.xp.maximum((model_d_quad_batch / self.c_val) * self.sigma_v, 1e-9)

        # 5. Vectorized likelihood computation (log PDF)
        # dL_gw_samples_exp shape: (N_gw_samples, 1, 1)
        # model_d_quad_batch_exp shape: (1, N_batch_marg, N_quad_points)
        # sigma_d_quad_batch_exp shape: (1, N_batch_marg, N_quad_points)
        log_pdf_quad = logpdf_normal_xp(
            self.xp,
            self.dL_gw_samples[:, self.xp.newaxis, self.xp.newaxis], # (N_gw_samples, 1, 1)
            loc=model_d_quad_batch[self.xp.newaxis, :, :],           # (1, N_batch_marg, N_quad_points)
            scale=sigma_d_quad_batch[self.xp.newaxis, :, :]          # (1, N_batch_marg, N_quad_points)
        ) # Result shape: (N_gw_samples, N_batch_marg, N_quad_points)

        # Handle NaNs from logpdf_normal_xp by converting them to -inf.
        # Also ensure any non-finite values (like +inf from a bad scale) become -inf.
        log_pdf_quad = self.xp.where(
            ~self.xp.isfinite(log_pdf_quad), -self.xp.inf, log_pdf_quad
        )
        
        # 6. Sum over GW samples (axis=0)
        # log_likelihood_quad shape: (N_batch_marg, N_quad_points)
        log_likelihood_quad = (logsumexp_xp(self.xp, log_pdf_quad, axis=0) -
                             self.xp.log(self.xp.array(len(self.dL_gw_samples), dtype=self.xp.float64)))
        
        # Apply overall_valid_points_mask: set log_likelihood_quad to -inf for invalid integration points.
        # This ensures that only quadrature points with valid z_quad and valid model_d_quad contribute.
        log_likelihood_quad = self.xp.where(
            overall_valid_points_mask, log_likelihood_quad, -self.xp.inf
        )

        # 7. Apply quadrature weights and integrate over quadrature points (axis=1)
        # self._quad_weights shape: (N_quad_points,)
        # log_weights_quad shape: (N_quad_points,)
        log_weights_quad = self.xp.log(self._quad_weights / self.xp.sqrt(self.xp.pi))
        
        # log_integrand_quad shape: (N_batch_marg, N_quad_points)
        # Add log_weights_quad (broadcasted from (N_quad_points,) to (1, N_quad_points))
        # to log_likelihood_quad (N_batch_marg, N_quad_points)
        log_integrand_quad = log_likelihood_quad + log_weights_quad[self.xp.newaxis, :]
        
        # Final integration using logsumexp over quadrature axis (axis=1)
        # marginalized_log_likelihoods_batch shape: (N_batch_marg,)
        marginalized_log_likelihoods_batch = logsumexp_xp(self.xp, log_integrand_quad, axis=1)
        
        # Ensure final results are finite, replace NaNs or Infs with -self.xp.inf
        marginalized_log_likelihoods_batch = self.xp.where(
            ~self.xp.isfinite(marginalized_log_likelihoods_batch), 
            -self.xp.inf, 
            marginalized_log_likelihoods_batch
        )
        
        return marginalized_log_likelihoods_batch

    def _compute_galaxy_likelihoods_looped(self, model_distances, distance_uncertainties, H0):
        """Compute galaxy likelihoods using memory-efficient looped approach.
        
        Args:
            model_distances: Luminosity distances for each galaxy
            distance_uncertainties: Distance uncertainties for each galaxy
            H0 (float): Hubble constant value
            
        Returns:
            array: Log-likelihood contribution for each galaxy, or None if computation fails
        """
        log_P_data_H0_zi_terms = self.xp.zeros(len(self.z_values))
        
        for i in range(len(self.z_values)):
            likelihood = self._compute_single_galaxy_likelihood_looped(
                i, model_distances[i], distance_uncertainties[i], H0
            )
            log_P_data_H0_zi_terms = self._update_array_element(
                log_P_data_H0_zi_terms, i, likelihood
            )
        
        return log_P_data_H0_zi_terms

    def _compute_single_galaxy_likelihood_looped(self, galaxy_idx, model_distance, distance_uncertainty, H0):
        """Compute likelihood for a single galaxy in looped mode.
        
        Args:
            galaxy_idx (int): Index of the galaxy
            model_distance (float): Model luminosity distance for this galaxy
            distance_uncertainty (float): Distance uncertainty for this galaxy
            H0 (float): Hubble constant value
            
        Returns:
            float: Log-likelihood contribution for this galaxy
        """
        mu_z = self.z_values[galaxy_idx]
        sigma_z = self.z_err_values[galaxy_idx]

        # Check if marginalization is needed
        if sigma_z < self.z_err_threshold:
            # Skip marginalization for essentially zero uncertainty
            return self._compute_simple_galaxy_likelihood(model_distance, distance_uncertainty)
        else:
            # Perform redshift marginalization
            return self._marginalize_single_galaxy_redshift_looped(
                mu_z, sigma_z, H0
            )

    def _compute_simple_galaxy_likelihood(self, model_distance, distance_uncertainty):
        """Compute likelihood for a single galaxy without redshift marginalization.
        
        Args:
            model_distance (float): Model luminosity distance
            distance_uncertainty (float): Distance uncertainty
            
        Returns:
            float: Log-likelihood value
        """
        try:
            log_pdf_for_one_galaxy = logpdf_normal_xp(
                self.xp,
                self.dL_gw_samples,
                loc=model_distance,
                scale=distance_uncertainty,
            )
        except Exception:
            return -self.xp.inf

        if self.xp.any(~self.xp.isfinite(log_pdf_for_one_galaxy)):
            return -self.xp.inf
        else:
            reduced_log_pdf = logsumexp_xp(self.xp, log_pdf_for_one_galaxy)
            return reduced_log_pdf - self.xp.log(self.xp.array(len(self.dL_gw_samples), dtype=self.xp.float64))

    def _marginalize_single_galaxy_redshift_looped(self, mu_z, sigma_z, H0):
        """Perform redshift marginalization for a single galaxy using looped quadrature.
        
        Args:
            mu_z (float): Mean redshift
            sigma_z (float): Redshift uncertainty  
            H0 (float): Hubble constant value
            
        Returns:
            float: Marginalized log-likelihood
        """
        # Compute integration bounds
        z_min = self.xp.maximum(mu_z - self.z_sigma_range * sigma_z, 1e-6)
        z_max = mu_z + self.z_sigma_range * sigma_z
        
        # Check if the integration range is meaningful
        if z_max <= z_min:
            # Fallback to point estimate
            model_distance = self._lum_dist_model(mu_z, H0)
            distance_uncertainty = self.xp.maximum((model_distance / self.c_val) * self.sigma_v, 1e-9)
            return self._compute_simple_galaxy_likelihood(model_distance, distance_uncertainty)

        # Gaussian quadrature integration
        return self._perform_looped_quadrature_integration(mu_z, sigma_z, H0)

    def _perform_looped_quadrature_integration(self, mu_z, sigma_z, H0):
        """Perform Gaussian quadrature integration using a loop over quadrature points.
        
        Args:
            mu_z (float): Mean redshift
            sigma_z (float): Redshift uncertainty
            H0 (float): Hubble constant value
            
        Returns:
            float: Integrated log-likelihood
        """
        log_integrand_values = []
        valid_evaluations = 0
        
        for node, weight in zip(self._quad_nodes, self._quad_weights):
            # Transform quadrature point to redshift domain
            z_j = mu_z + self.xp.sqrt(2.0) * sigma_z * node
            
            # Skip negative redshifts
            if z_j <= 0:
                continue
                
            try:
                likelihood_at_point = self._evaluate_likelihood_at_redshift_point(z_j, H0, weight)
                if likelihood_at_point is not None:
                    log_integrand_values.append(likelihood_at_point)
                    valid_evaluations += 1
            except Exception:
                # Skip this quadrature point if evaluation fails
                continue

        # Combine the integration results
        if valid_evaluations > 0 and len(log_integrand_values) > 0:
            log_integrand_array = self.xp.array(log_integrand_values)
            return logsumexp_xp(self.xp, log_integrand_array)
        else:
            # Integration failed - return -inf
            return -self.xp.inf

    def _evaluate_likelihood_at_redshift_point(self, z_j, H0, weight):
        """Evaluate likelihood at a specific redshift point for quadrature integration.
        
        Args:
            z_j (float): Redshift value
            H0 (float): Hubble constant value
            weight (float): Quadrature weight
            
        Returns:
            float: Log of weighted likelihood at this point, or None if evaluation fails
        """
        # Compute luminosity distance for this redshift
        model_d = self._lum_dist_model(z_j, H0)
        
        # Check if model distance is reasonable
        if not self.xp.isfinite(model_d) or model_d <= 0:
            return None
            
        sigma_d = self.xp.maximum((model_d / self.c_val) * self.sigma_v, 1e-9)
        
        # Compute likelihood for this redshift
        log_pdf = logpdf_normal_xp(
            self.xp,
            self.dL_gw_samples,
            loc=model_d,
            scale=sigma_d,
        )
        
        if self.xp.any(~self.xp.isfinite(log_pdf)):
            return None
        
        # Sum over GW samples
        current_logsumexp = logsumexp_xp(self.xp, log_pdf)
        log_likelihood_at_z = current_logsumexp - self.xp.log(self.xp.array(len(self.dL_gw_samples), dtype=self.xp.float64))
        
        # Store log(weight * likelihood) for numerical stability
        log_weight = self.xp.log(weight / self.xp.sqrt(self.xp.pi))
        return log_weight + log_likelihood_at_z

    def _compute_galaxy_weights(self, alpha):
        """Compute galaxy weights based on mass proxy and alpha parameter.
        
        Args:
            alpha (float): Mass bias parameter
            
        Returns:
            array: Normalized weights for each galaxy, or None if computation fails
        """
        if self.xp.isclose(alpha, 0.0):
            # Equal weights when alpha = 0
            return self.xp.full(len(self.mass_proxy_values), 1.0 / len(self.mass_proxy_values))
        else:
            # Mass-weighted based on alpha
            powered = self.mass_proxy_values ** alpha
            if self.xp.any(powered <= 0) or not self.xp.all(self.xp.isfinite(powered)):
                return None
            denom = powered.sum()
            if not self.xp.isfinite(denom) or denom <= 0:
                return None
            return powered / denom

    def _combine_weighted_likelihoods(self, galaxy_log_likelihoods, galaxy_weights):
        """Combine galaxy likelihoods with their weights to compute final likelihood.
        
        Args:
            galaxy_log_likelihoods: Log-likelihood contribution from each galaxy
            galaxy_weights: Weight for each galaxy based on mass proxy
            
        Returns:
            float: Final weighted log-likelihood
        """
        valid_mask = galaxy_weights > 0
        if not self.xp.any(valid_mask):
            return -self.xp.inf
        
        terms_to_sum = self.xp.log(galaxy_weights[valid_mask]) + galaxy_log_likelihoods[valid_mask]
        total_log_likelihood = logsumexp_xp(self.xp, terms_to_sum)

        if not self.xp.isfinite(total_log_likelihood):
            return -self.xp.inf

        return total_log_likelihood

    def _update_array_element(self, array, index, value):
        """Update array element in a backend-agnostic way.
        
        Args:
            array: Array to update
            index: Index to update
            value: New value
            
        Returns:
            Updated array
        """
        if self.backend_name == "jax":
            # JAX arrays are immutable, use at() method
            return array.at[index].set(value)
        else:
            # NumPy arrays are mutable
            array[index] = value
            return array
    
    def get_distance_cache_stats(self):
        """Get distance cache performance statistics.
        
        Returns:
            dict: Cache statistics including hit rate, build times, etc.
        """
        return self._distance_cache.get_statistics()
    
    def log_distance_cache_stats(self):
        """Log distance cache performance statistics."""
        self._distance_cache.log_statistics()
    
    def validate_distance_cache_accuracy(self, max_relative_error=1e-4):
        """Validate distance cache accuracy against direct computation.
        
        Args:
            max_relative_error: Maximum acceptable relative error
            
        Returns:
            bool: True if validation passes
        """
        # Create test points covering the typical range
        test_z_values = np.array([0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0])
        test_h0_values = np.array([50.0, 65.0, 70.0, 75.0, 90.0, 120.0])
        
        return self._distance_cache.validate_accuracy(
            test_z_values, test_h0_values, max_relative_error
        )
    
    def clear_distance_cache(self):
        """Clear the distance cache and reset statistics."""
        self._distance_cache.clear_cache()

def get_log_likelihood_h0(
    requested_backend_str: str,
    dL_gw_samples,
    host_galaxies_z,
    host_galaxies_mass_proxy,
    host_galaxies_z_err,
    sigma_v=DEFAULT_SIGMA_V_PEC,
    c_val=DEFAULT_C_LIGHT,
    omega_m_val=DEFAULT_OMEGA_M,
    h0_min=DEFAULT_H0_PRIOR_MIN,
    h0_max=DEFAULT_H0_PRIOR_MAX,
    alpha_min=DEFAULT_ALPHA_PRIOR_MIN,
    alpha_max=DEFAULT_ALPHA_PRIOR_MAX,
    z_err_threshold=DEFAULT_Z_ERR_THRESHOLD,
    n_quad_points=DEFAULT_QUAD_POINTS,
    z_sigma_range=DEFAULT_Z_MARGINALIZATION_SIGMA_RANGE,
    force_non_vectorized=False,
    batch_size=DEFAULT_MCMC_BATCH_SIZE,
):
    """Create and configure an H0LogLikelihood instance with the specified backend.
    
    This is the main factory function for creating likelihood objects. It handles
    backend selection, memory optimization decisions, and proper configuration.
    
    Args:
        requested_backend_str (str): The backend to use ("auto", "numpy", "jax")
        dL_gw_samples: Gravitational wave luminosity distance samples
        host_galaxies_z: Redshifts of candidate host galaxies
        host_galaxies_mass_proxy: Mass proxy values for galaxies
        host_galaxies_z_err: Redshift uncertainties for galaxies
        ... (other parameters as before)
        force_non_vectorized (bool): If True, forces non-vectorized (now batched) path.
        batch_size (int): Batch size for batched likelihood computation.
        
    Returns:
        H0LogLikelihood: Configured likelihood instance
    """
    # 1. Initialize backend
    backend_config = _initialize_backend(requested_backend_str)
    
    # 2. Analyze data dimensions
    data_dimensions = _analyze_data_dimensions(dL_gw_samples, host_galaxies_z)
    
    # 3. Determine vectorization strategy
    vectorization_config = _determine_vectorization_strategy(
        backend_config, data_dimensions, force_non_vectorized
    )
    
    # 4. Log configuration decisions
    _log_likelihood_configuration(backend_config, data_dimensions, vectorization_config)
    
    # 5. Create and return likelihood instance
    return H0LogLikelihood(
        backend_config.xp_module,
        backend_config.backend_name,
        dL_gw_samples,
        host_galaxies_z,
        host_galaxies_mass_proxy,
        host_galaxies_z_err,
        sigma_v,
        c_val,
        omega_m_val,
        h0_min,
        h0_max,
        alpha_min,
        alpha_max,
        use_vectorized_likelihood=vectorization_config.should_use_vectorized,
        z_err_threshold=z_err_threshold,
        n_quad_points=n_quad_points,
        z_sigma_range=z_sigma_range,
        batch_size=batch_size,
    )

# Supporting data classes for cleaner interfaces
@dataclass
class BackendConfig:
    """Configuration for computational backend."""
    xp_module: object
    backend_name: str
    device_name: str

@dataclass  
class DataDimensions:
    """Analysis of input data dimensions."""
    n_gw_samples: int
    n_hosts: int
    total_elements: int

@dataclass
class VectorizationConfig:
    """Configuration for vectorization strategy."""
    should_use_vectorized: bool
    memory_threshold_elements: float
    force_non_vectorized: bool

def _initialize_backend(requested_backend_str: str) -> BackendConfig:
    """Initialize and configure the computational backend.
    
    Args:
        requested_backend_str: Backend identifier ("auto", "numpy", "jax")
        
    Returns:
        BackendConfig: Backend configuration object
    """
    xp_module, backend_name, device_name = get_xp(requested_backend_str)
    return BackendConfig(
        xp_module=xp_module,
        backend_name=backend_name, 
        device_name=device_name
    )

def _analyze_data_dimensions(dL_gw_samples, host_galaxies_z) -> DataDimensions:
    """Analyze input data dimensions for memory and performance planning.
    
    Args:
        dL_gw_samples: GW distance samples
        host_galaxies_z: Host galaxy redshifts
        
    Returns:
        DataDimensions: Analysis of data sizes
    """
    # Ensure array-like inputs for length calculation
    n_gw_samples = len(np.asarray(dL_gw_samples))
    
    # Handle scalar vs array host redshifts
    host_z_array = np.asarray(host_galaxies_z)
    if host_z_array.ndim == 0:
        n_hosts = 1
    else:
        n_hosts = len(host_z_array)
    
    total_elements = n_gw_samples * n_hosts
    
    return DataDimensions(
        n_gw_samples=n_gw_samples,
        n_hosts=n_hosts,
        total_elements=total_elements
    )

def _determine_vectorization_strategy(
    backend_config: BackendConfig, 
    data_dimensions: DataDimensions,
    force_non_vectorized: bool
) -> VectorizationConfig:
    """Determine optimal vectorization strategy based on backend and data size.
    
    Args:
        backend_config: Backend configuration
        data_dimensions: Data dimension analysis
        force_non_vectorized: Override flag to force looped computation
        
    Returns:
        VectorizationConfig: Vectorization strategy configuration
    """
    memory_threshold_elements = MEMORY_THRESHOLD_BYTES / BYTES_PER_ELEMENT
    
    if backend_config.backend_name == "jax":
        # JAX benefits from vectorization unless explicitly overridden
        should_use_vectorized = not force_non_vectorized
    else:
        # NumPy: use memory-based decision unless overridden
        memory_allows_vectorization = data_dimensions.total_elements <= memory_threshold_elements
        should_use_vectorized = memory_allows_vectorization and not force_non_vectorized
    
    return VectorizationConfig(
        should_use_vectorized=should_use_vectorized,
        memory_threshold_elements=memory_threshold_elements,
        force_non_vectorized=force_non_vectorized
    )

def _log_likelihood_configuration(
    backend_config: BackendConfig,
    data_dimensions: DataDimensions, 
    vectorization_config: VectorizationConfig
):
    """Log the likelihood configuration decisions for debugging and monitoring.
    
    Args:
        backend_config: Backend configuration
        data_dimensions: Data dimensions
        vectorization_config: Vectorization configuration
    """
    if backend_config.backend_name == "jax":
        if vectorization_config.force_non_vectorized:
            logger.info(f"JAX backend FORCED to non-vectorized for testing/debugging purposes.")
        else:
            logger.info(f"Using JAX backend (on {backend_config.device_name}). Will use JAX-optimized likelihood path.")
    else:
        # NumPy backend logging
        memory_gb = MEMORY_THRESHOLD_BYTES / (1024**3)
        
        if vectorization_config.force_non_vectorized:
            logger.info(f"FORCED non-vectorized likelihood for testing/debugging purposes.")
        
        if vectorization_config.should_use_vectorized:
            logger.info(
                f"Using NumPy backend, VECTORIZED likelihood: N_samples ({data_dimensions.n_gw_samples}) * "
                f"N_hosts ({data_dimensions.n_hosts}) = {data_dimensions.total_elements} elements. "
                f"Threshold: {vectorization_config.memory_threshold_elements:.0f} elements ({memory_gb:.0f} GB)."
            )
        else:
            reason = "Forced by force_non_vectorized=True" if vectorization_config.force_non_vectorized else \
                    f"Exceeds threshold of {vectorization_config.memory_threshold_elements:.0f} elements ({memory_gb:.0f} GB)"
            logger.info(
                f"Using NumPy backend, LOOPED likelihood (memory efficient): N_samples ({data_dimensions.n_gw_samples}) * "
                f"N_hosts ({data_dimensions.n_hosts}) = {data_dimensions.total_elements} elements. {reason}."
            )

def run_mcmc_h0(
    log_likelihood_func,
    event_name,
    n_walkers=DEFAULT_MCMC_N_WALKERS,
    n_dim=DEFAULT_MCMC_N_DIM,
    initial_h0_mean=DEFAULT_MCMC_INITIAL_H0_MEAN,
    initial_h0_std=DEFAULT_MCMC_INITIAL_H0_STD,
    alpha_prior_min=DEFAULT_ALPHA_PRIOR_MIN,
    alpha_prior_max=DEFAULT_ALPHA_PRIOR_MAX,
    n_steps=DEFAULT_MCMC_N_STEPS,
    pool=None,
):
    """Run MCMC sampling for H0 and alpha parameters.
    
    This function orchestrates the entire MCMC process: initialization,
    execution, and error handling.
    
    Args:
        log_likelihood_func: The log likelihood function
        event_name: Name of the event for logging
        n_walkers: Number of MCMC walkers
        n_dim: Number of dimensions (2 for H0 and alpha)
        initial_h0_mean: Mean for H0 walker initialization
        initial_h0_std: Standard deviation for H0 initialization
        alpha_prior_min: Lower bound for alpha initialization
        alpha_prior_max: Upper bound for alpha initialization  
        n_steps: Number of MCMC steps
        pool: Optional pool object for parallelization
        
    Returns:
        emcee.EnsembleSampler: The sampler object after running, or None on failure
    """
    logger.info(f"Running MCMC for H0 and alpha on event {event_name} ({n_steps} steps, {n_walkers} walkers)...")
    
    # 1. Initialize walker positions
    initial_positions = _initialize_mcmc_walkers(
        n_walkers, initial_h0_mean, initial_h0_std, alpha_prior_min, alpha_prior_max
    )
    
    # 2. Create and configure sampler
    sampler = _create_mcmc_sampler(n_walkers, n_dim, log_likelihood_func, pool)
    
    # 3. Run MCMC with error handling
    return _execute_mcmc_run(sampler, initial_positions, n_steps, event_name)

def _initialize_mcmc_walkers(n_walkers, h0_mean, h0_std, alpha_min, alpha_max):
    """Initialize walker positions for MCMC sampling.
    
    Args:
        n_walkers: Number of walkers
        h0_mean: Mean H0 value for initialization
        h0_std: Standard deviation for H0 initialization
        alpha_min: Minimum alpha value
        alpha_max: Maximum alpha value
        
    Returns:
        np.array: Initial walker positions (n_walkers, 2)
    """
    # Initialize H0 positions around the mean
    pos_H0 = h0_mean + h0_std * np.random.randn(n_walkers, 1)
    
    # Initialize alpha positions uniformly within prior range
    pos_alpha = np.random.uniform(alpha_min, alpha_max, size=(n_walkers, 1))
    
    # Combine into full parameter space
    return np.hstack((pos_H0, pos_alpha))

def _create_mcmc_sampler(n_walkers, n_dim, log_likelihood_func, pool):
    """Create and configure the MCMC sampler.
    
    Args:
        n_walkers: Number of walkers
        n_dim: Number of dimensions  
        log_likelihood_func: Log likelihood function
        pool: Optional parallelization pool
        
    Returns:
        emcee.EnsembleSampler: Configured sampler
    """
    return emcee.EnsembleSampler(
        n_walkers, 
        n_dim, 
        log_likelihood_func, 
        moves=emcee.moves.StretchMove(),
        pool=pool
    )

def _execute_mcmc_run(sampler, initial_positions, n_steps, event_name):
    """Execute the MCMC run with comprehensive error handling.
    
    Args:
        sampler: Configured MCMC sampler
        initial_positions: Initial walker positions
        n_steps: Number of steps to run
        event_name: Event name for logging
        
    Returns:
        emcee.EnsembleSampler: Sampler after running, or None on failure
    """
    try:
        sampler.run_mcmc(initial_positions, n_steps, progress=True)
        logger.info(f"MCMC run completed successfully for {event_name}.")
        return sampler
        
    except ValueError as ve:
        logger.error(f"❌ ValueError during MCMC for {event_name}: {ve}")
        logger.error("  This can happen if the likelihood consistently returns -inf. Check priors or input data.")
        return None
        
    except Exception as e:
        logger.exception(f"❌ An unexpected error occurred during MCMC for {event_name}: {e}")
        return None

def process_mcmc_samples(sampler, event_name, burnin=DEFAULT_MCMC_BURNIN, thin_by=DEFAULT_MCMC_THIN_BY, n_dim=DEFAULT_MCMC_N_DIM):
    """Process MCMC samples by applying burn-in, thinning, and validation.
    
    Args:
        sampler: MCMC sampler object after running
        event_name: Name of the event for logging  
        burnin: Number of burn-in steps to discard
        thin_by: Factor to thin the samples by
        n_dim: Expected number of dimensions in the chain
        
    Returns:
        np.array: Processed samples, or None if processing fails
    """
    if sampler is None:
        logger.warning(f"⚠️ Sampler object is None for {event_name}. Cannot process MCMC samples.")
        return None

    logger.info(f"Processing MCMC samples for {event_name} (burn-in: {burnin}, thin: {thin_by})...")
    
    try:
        # 1. Extract and validate raw samples
        raw_samples = _extract_raw_samples(sampler, burnin, thin_by)
        if raw_samples is None:
            return None
            
        # 2. Validate sample dimensions and content
        if not _validate_sample_dimensions(raw_samples, event_name, sampler):
            return None
            
        # 3. Format samples according to expected dimensions
        return _format_processed_samples(raw_samples, n_dim, event_name)
        
    except Exception as e:
        logger.exception(f"❌ Error processing MCMC chain for {event_name}: {e}")
        return None

def _extract_raw_samples(sampler, burnin, thin_by):
    """Extract raw samples from sampler with burn-in and thinning.
    
    Args:
        sampler: MCMC sampler object
        burnin: Number of burn-in steps
        thin_by: Thinning factor
        
    Returns:
        np.array: Raw flattened samples, or None if extraction fails
    """
    # Extract samples: shape (n_steps, n_walkers, n_dim) -> (n_samples_total, n_dim)
    flat_samples = sampler.get_chain(discard=burnin, thin=thin_by, flat=True)
    return flat_samples

def _validate_sample_dimensions(samples, event_name, sampler):
    """Validate that extracted samples have reasonable dimensions.
    
    Args:
        samples: Extracted samples array
        event_name: Event name for logging
        sampler: Original sampler for diagnostics
        
    Returns:
        bool: True if samples are valid, False otherwise
    """
    if samples.shape[0] == 0:
        original_length = sampler.get_chain().shape[0]
        logger.warning(
            f"⚠️ MCMC for {event_name} resulted in NO valid samples after burn-in and thinning."
        )
        logger.warning(f"  Original chain length before discard: {original_length}")
        return False
    return True

def _format_processed_samples(samples, expected_n_dim, event_name):
    """Format processed samples according to expected dimensions.
    
    Args:
        samples: Raw processed samples
        expected_n_dim: Expected number of dimensions
        event_name: Event name for logging
        
    Returns:
        np.array: Properly formatted samples, or None if formatting fails
    """
    # Handle different dimensional cases
    if expected_n_dim == 1 and samples.ndim == 2 and samples.shape[1] == 1:
        # Single parameter case - extract the column
        processed_samples = samples[:, 0]
    elif expected_n_dim > 1 and samples.ndim == 2 and samples.shape[1] == expected_n_dim:
        # Multi-dimensional case - keep as is
        logger.info(f"  Note: MCMC had {expected_n_dim} dimensions. Returning all dimensions after processing.")
        processed_samples = samples
    elif samples.ndim == 1 and expected_n_dim == 1:
        # Already 1D for single parameter
        processed_samples = samples
    else:
        # Unexpected shape
        logger.warning(
            f"⚠️ Unexpected shape for flat_samples: {samples.shape}. Expected ({'*', expected_n_dim}). "
            f"Cannot safely extract parameters."
        )
        return None

    logger.info(f"  Successfully processed MCMC samples for {event_name}. Number of samples: {len(processed_samples)}.")
    return processed_samples

def profile_likelihood_call(log_like_func_to_profile, h0_values_to_test, n_calls=10):
    """Helper function to profile the likelihood call multiple times."""
    logger.info(f"Profiling {log_like_func_to_profile.__class__.__name__} ({'vectorized' if getattr(log_like_func_to_profile, 'use_vectorized_likelihood', False) else 'looped'})...")
    for h0_val in h0_values_to_test:
        for _ in range(n_calls):
            _ = log_like_func_to_profile([h0_val]) # Call the likelihood